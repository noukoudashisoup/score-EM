{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scem import gen, kernel, ebm, net\n",
    "from scem import util\n",
    "import scem.loss as scem_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('lines', linewidth=2, markersize=10)\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.double)\n",
    "torch.manual_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalMixture(nn.Module):\n",
    "    def __init__(self, dh1, dh2, dout, dnoise,\n",
    "                 n_classes, n_logits, temperature=1.):\n",
    "        super(CategoricalMixture, self).__init__()\n",
    "        self.dout = dout\n",
    "        self.dnoise = dnoise\n",
    "        self.n_logits = n_logits\n",
    "        self.n_classes = n_classes\n",
    "        self.feat = net.TwoLayerFC(dnoise, dh1, dh2, dout)\n",
    "        self.mlinear = net.MultipleLinear(dout, n_classes, n_logits,\n",
    "                                          bias=True)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, noise):\n",
    "        return (self.feat(noise))\n",
    "\n",
    "    def sample_noise(self, n_sample, seed=14):\n",
    "        noise = torch.randn(n_sample, self.dnoise)\n",
    "        return noise\n",
    "    \n",
    "    def in_out_shapes(self):\n",
    "        return ((self.dnoise,), self.dout) \n",
    "\n",
    "    def sample(self, n_sample, seed=13):\n",
    "        noise = self.sample_noise(n_sample, seed)\n",
    "        out = self.forward(noise).relu()\n",
    "        logits = self.mlinear(out)\n",
    "        if self.training:\n",
    "            m = dists.RelaxedOneHotCategorical(\n",
    "                self.temperature,\n",
    "                logits=logits,\n",
    "            )\n",
    "            sample = m.rsample()\n",
    "            # print(sample)\n",
    "            return sample\n",
    "        m = dists.OneHotCategorical(logits=logits)\n",
    "        sample = m.sample()\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical(nn.Module):\n",
    "    def __init__(self, n_classes, n_logits, temperature=1.):\n",
    "        super(Categorical, self).__init__()\n",
    "        self.n_logits = n_logits\n",
    "        self.n_classes = n_classes\n",
    "        self.logits = nn.Parameter(\n",
    "            torch.Tensor(n_logits, n_classes))\n",
    "        self.logits = nn.init.normal_(self.logits) \n",
    "        self.temperature = temperature\n",
    "\n",
    "    def sample(self, n_sample, seed=13):\n",
    "        logits = self.logits\n",
    "        with util.TorchSeedContext(seed):\n",
    "            if self.training:\n",
    "                m = dists.RelaxedOneHotCategorical(\n",
    "                    self.temperature,\n",
    "                    logits=logits,\n",
    "                )\n",
    "                sample = m.rsample([n_sample])\n",
    "                # print(sample)\n",
    "                return sample\n",
    "            m = dists.OneHotCategorical(logits=logits)\n",
    "            sample = m.sample([n_sample])\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralMachine(ebm.LatentEBM):\n",
    "    \n",
    "    var_type_latent = None\n",
    "    var_type_obs = None\n",
    "\n",
    "    def __init__(self, din, emb_d, n_classes, d1=10, d2=10):\n",
    "        super(NeuralMachine, self).__init__()\n",
    "        self.din = din\n",
    "        self.emb_d = emb_d\n",
    "        self.n_classs = n_classes\n",
    "        \n",
    "        self.W = nn.Parameter(\n",
    "            torch.Tensor(emb_d, n_classes))\n",
    "        self.W = nn.init.normal_(self.W) \n",
    "        self.lin1 = nn.Linear(emb_d*din, d1)\n",
    "        self.lin2 = nn.Linear(d1, d2)\n",
    "        self.lin3 = nn.Linear(d2, 1)\n",
    "\n",
    "    def forward(self, X, Z):\n",
    "        W = self.W\n",
    "        Y = torch.einsum('ijk, dk->ijd', X, W, )\n",
    "        Y = Y.reshape(Y.shape[0], -1)\n",
    "        Y = self.lin1(Y).relu()\n",
    "        Y = self.lin2(Y).relu()\n",
    "        Y = self.lin3(Y).tanh().squeeze()\n",
    "        return Y\n",
    "    \n",
    "    def score_marginal_obs(self, X):\n",
    "        D = util.forward_diff_onehot(self.forward, 0,\n",
    "                                     [X, None])\n",
    "        return torch.exp(D) - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blzm = NeuralMachine(din=dx, emb_d=3, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CategoricalMixture(100, 10, 10, 30, n_classes=2, n_logits=dx, temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Categorical(n_classes=2, n_logits=dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = kernel.OHKGauss(2, torch.tensor(dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksd = scem_loss.KSD(k, blzm.score_marginal_obs)\n",
    "iksd = scem_loss.IncompleteKSD(k, blzm.score_marginal_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_cm = torch.optim.Adam(cm.parameters(), lr=1e-3,\n",
    "                          weight_decay=0.)\n",
    "opt_c = torch.optim.Adam(c.parameters(), lr=1e-3,\n",
    "                          weight_decay=0.)\n",
    "niter = 300\n",
    "n = 300\n",
    "test_n = 300\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(batch_size, X, detach=True):\n",
    "    perm = torch.randperm(X.shape[0]).detach()\n",
    "    idx = perm[:batch_size]\n",
    "    X_ = X[idx]\n",
    "    if detach:\n",
    "        X_ = X_.detach()\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(niter):\n",
    "    X = cm.sample(n, seed=0)\n",
    "    # loss = ksd.loss(X)\n",
    "    i1, i2 = util.sample_incomplete_ustat_batch(n, batch_size)\n",
    "    loss = iksd.loss(X[i1], X[i2])\n",
    "    opt_cm.zero_grad()\n",
    "    loss.backward(retain_graph=False)\n",
    "    opt_cm.step()   \n",
    "    if i % 1 == 0:\n",
    "        cm.eval()\n",
    "        X_ = cm.sample(test_n, seed=i)\n",
    "        test_loss = ksd.loss(X_)\n",
    "        cm.train()\n",
    "        print(test_loss.item())\n",
    "        losses += [test_loss.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_losses = []\n",
    "for i in range(niter):\n",
    "    X = c.sample(n, seed=0)\n",
    "    loss = ksd.loss(X)\n",
    "    opt_c.zero_grad()\n",
    "    loss.backward(retain_graph=False)\n",
    "    opt_c.step()   \n",
    "    if i % 1 == 0:\n",
    "        c.eval()\n",
    "        X_ = c.sample(test_n, seed=i)\n",
    "        test_loss = ksd.loss(X_)\n",
    "        c.train()\n",
    "        print(test_loss.item())\n",
    "        c_losses += [test_loss.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "losses_ = [np.abs(l) for l in losses]\n",
    "c_losses_ = [np.abs(l) for l in c_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(c_losses_, label='independent')\n",
    "plt.plot(losses_, label='mixture')\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('test KSD')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
