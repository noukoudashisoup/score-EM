{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3, linewidth=120)\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from scem import ebm, stein, kernel, util, gen\n",
    "from scem.datasets import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = \"banana\"\n",
    "p = load_data(dname, D=2, noise_std = 0.0, seed=0, itanh=False, whiten=False )\n",
    "\n",
    "x = p.sample(1000)\n",
    "x_eval = p.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "import numpy as np\n",
    "import torch.distributions as td\n",
    "torch.random.manual_seed(13)\n",
    "\n",
    "class EBM(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    EBM \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, Dx, Dz, Dh):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(Dz+Dx, Dh)\n",
    "        self.layer_2 = nn.Linear(Dh, 1)\n",
    "        self.elu = nn.ELU()\n",
    "    \n",
    "    def forward(self, X, Z):\n",
    "\n",
    "        XZ = torch.cat([X, Z], axis=-1)\n",
    "        #h  = self.elu(self.layer_1(XZ))\n",
    "        h = torch.relu(self.layer_1(XZ))\n",
    "        E  = self.layer_2(h)\n",
    "        return E[:,0]\n",
    "    \n",
    "\n",
    "# dimensionality of model \n",
    "Dx = 2\n",
    "Dz = 2\n",
    "Dh = 100\n",
    "\n",
    "lebm = ebm.LatentEBMAdapter(EBM(Dx, Dz, Dh), var_type_obs='continuous', var_type_latent='continuous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out_shapes = ((10,), (100,))\n",
    "in_shape = in_out_shapes[0]\n",
    "def noise_sampler(n_sample, n, seed=1):\n",
    "    with util.TorchSeedContext(seed):\n",
    "        N = torch.randn(n_sample, n, in_shape[0])\n",
    "    return N\n",
    "\n",
    "dh = 100\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lin1 = nn.Linear(Dx+in_shape[0], dh)\n",
    "        self.lin2 = nn.Linear(dh, Dz)\n",
    "        self.module = nn.Sequential(\n",
    "            self.lin1,\n",
    "            nn.ReLU(),\n",
    "            # nn.ELU(),\n",
    "            self.lin2,\n",
    "        )\n",
    "        \n",
    "    def forward(self, noise, X):\n",
    "        n_sample = noise.shape[0]\n",
    "        X_ = torch.stack([X]*n_sample)\n",
    "        Y = torch.cat([X_, noise], axis=-1)\n",
    "        return self.module.forward(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.as_tensor(x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel\n",
    "\n",
    "med2 = util.pt_meddistance(X)**2\n",
    "kx = kernel.KIMQ(b=-0.5, c=1, s2=1.)\n",
    "#kx = kernel.KGauss(torch.tensor([med2]))\n",
    "#kx = kernel.KGauss(torch.tensor(med2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q(z|x)\n",
    "#cs = gen.CSFactorisedGaussian(Dx, Dz, Dh)\n",
    "cs = gen.CSNoiseTransformerAdapter(Net(), noise_sampler, in_out_shapes)\n",
    "\n",
    "# optimizer settings\n",
    "learning_rate_q = 1e-2\n",
    "weight_decay_q = 0# 1e-3\n",
    "optimizer_q = torch.optim.Adam(cs.parameters(), lr=learning_rate_q,\n",
    "                               weight_decay=weight_decay_q)\n",
    "\n",
    "approx_score = stein.ApproximateScore(\n",
    "        lebm.score_joint_obs, cs)\n",
    "approx_score.n_sample = 500\n",
    "\n",
    "# optimizer settings for p(x)\n",
    "learning_rate_p = 1e-1 # !!!\n",
    "weight_decay_p = 0.\n",
    "optimizer_p = torch.optim.Adam(lebm.parameters(), lr=learning_rate_p,\n",
    "                               weight_decay=weight_decay_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_p = 500\n",
    "iter_q = 10\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_loop(niter, X, cs, opt):\n",
    "    for i in range(niter):\n",
    "        Z = cs.sample(1, X)\n",
    "        Z = Z.squeeze(0)\n",
    "        zmed2 = util.pt_meddistance(Z)**2\n",
    "        kz = kernel.KIMQ(b=-0.5, c=1, s2=1.)\n",
    "        loss = stein.kcsd_ustat(\n",
    "            X, Z, lebm.score_joint_latent, kx, kz)\n",
    "        opt.zero_grad()\n",
    "        loss.backward(retain_graph=False)\n",
    "        opt.step()   \n",
    "    #print('kcsd', loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10812857747077942 0.10262227803468704\n",
      "0.03973084315657616 0.037283483892679214\n"
     ]
    }
   ],
   "source": [
    "#inner_loop(400, X)\n",
    "losses = []\n",
    "\n",
    "for t in range(iter_p):\n",
    "    # reset q(z|x)'s weight\n",
    "#     cs.apply(weight_reset)\n",
    "#     optimizer_q = torch.optim.Adam(cs.parameters(), lr=learning_rate_q,\n",
    "#                                weight_decay=weight_decay_q)\n",
    "    \n",
    "    perm = torch.randperm(X.shape[0]).detach()\n",
    "    idx = perm[:batch_size]\n",
    "    X_ = X[idx].detach()\n",
    "\n",
    "    inner_loop(iter_q, X_, cs, optimizer_q)\n",
    "    loss = stein.ksd_ustat(X_, approx_score, kx)  \n",
    "    losses += [loss.item()]\n",
    "\n",
    "    if (t%100 == 0):\n",
    "        loss_ = stein.ksd_ustat(X, approx_score, kx).item()\n",
    "        print(loss.item(), loss_)\n",
    "    \n",
    "    optimizer_p.zero_grad()\n",
    "    loss.backward(retain_graph=False)\n",
    "    optimizer_p.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a grid for numerical normalisation\n",
    "from itertools import product\n",
    "ngrid = 50\n",
    "grid = torch.linspace(-10, 10, ngrid)\n",
    "xz_eval = torch.tensor(list(product(*[grid]*4)))\n",
    "x_eval = xz_eval[:,:2]\n",
    "z_eval = xz_eval[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true log density\n",
    "E_true = p.logpdf_multiple(torch.tensor(list(product(*[grid]*2))))\n",
    "E_true -= E_true.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EBM log density\n",
    "E_eval = lebm(x_eval, z_eval).reshape(ngrid,ngrid,ngrid,ngrid).exp().detach()\n",
    "E_eval /= E_eval.sum()\n",
    "E_eval = E_eval.sum(-1).sum(-1)\n",
    "E_eval.log_()\n",
    "E_eval -= E_eval.max()\n",
    "# E_eval = E_eval.sum(-1).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(E):\n",
    "    if isinstance(E, np.ndarray):\n",
    "        E = np.exp(E)\n",
    "    else:\n",
    "        E = E.exp()\n",
    "    E /= E.sum()\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2,figsize=(6,6), sharex=True, sharey=True)\n",
    "\n",
    "\n",
    "ax = axes[0,0]\n",
    "ax.pcolor(grid, grid,E_true.reshape(ngrid,ngrid), shading='auto', vmin=-10, vmax=0)\n",
    "ax.scatter(x[:,1], x[:,0], c=\"r\", s=1, alpha=0.05)\n",
    "\n",
    "\n",
    "ax = axes[1,0]\n",
    "ax.pcolor(grid, grid,normalise(E_true).reshape(ngrid,ngrid), shading='auto')\n",
    "\n",
    "ax = axes[0,1]\n",
    "ax.pcolor(grid, grid,E_eval,shading='auto', vmin=-10, vmax=0, )\n",
    "ax.scatter(x[:,1], x[:,0], c=\"r\", s=1, alpha=0.05)\n",
    "\n",
    "ax = axes[1,1]\n",
    "ax.pcolor(grid, grid,normalise(E_eval),shading='auto' )\n",
    "ax.scatter(x[:,1], x[:,0], c=\"r\", s=1, alpha=0.0)\n",
    "\n",
    "\n",
    "\n",
    "axes[0,0].set_ylabel(\"logp\")\n",
    "axes[1,0].set_ylabel(\"logp\")\n",
    "\n",
    "axes[0,0].set_title(\"data\")\n",
    "axes[0,1].set_title(\"KSD\")\n",
    "\n",
    "axes[0,0].set_xlim(-10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (score_EM)",
   "language": "python",
   "name": "score_em"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
