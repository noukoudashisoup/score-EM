{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3, linewidth=120)\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from scem.datasets import *\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.distributions as td\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = \"banana\"\n",
    "p = load_data(dname, D=2, noise_std = 0.0, seed=0, itanh=False, whiten=False )\n",
    "\n",
    "x = p.sample(1000)\n",
    "x_eval = p.sample(100)\n",
    "\n",
    "softplus = nn.functional.softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.distributions as td\n",
    "\n",
    "\n",
    "class EBM(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    EBM \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, Dx, Dz, Dh):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(Dz+Dx, Dh)\n",
    "        self.layer_2 = nn.Linear(Dh, Dh)\n",
    "        self.layer_out = nn.Linear(Dh, 1)\n",
    "    \n",
    "    def forward(self, X, Z):\n",
    "        \n",
    "        if X.ndim == Z.ndim-1:\n",
    "            X = torch.stack([X]*Z.shape[0], 0)\n",
    "        \n",
    "        XZ = torch.cat([X, Z], axis=-1)\n",
    "        h  = torch.relu(self.layer_1(XZ))\n",
    "        h  = torch.relu(self.layer_2(h))\n",
    "        E  = self.layer_out(h)\n",
    "        return E[...,0]\n",
    "    \n",
    "    \n",
    "class Posterior(nn.Module):\n",
    "    '''\n",
    "    Posterior is a factorised normal\n",
    "    '''\n",
    "    def __init__(self, Dx, Dh, Dz):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(Dx, Dh)\n",
    "        self.layer_2 = nn.Linear(Dh, Dh)\n",
    "        self.layer_2_m = nn.Linear(Dh, Dz)\n",
    "        self.layer_2_v = nn.Linear(Dh, Dz)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        h = self.layer_1(x).tanh()\n",
    "        h = self.layer_2(h).tanh()\n",
    "        m = self.layer_2_m(h)\n",
    "        v = self.layer_2_v(h)\n",
    "        v = nn.functional.softplus(v)\n",
    "        \n",
    "        return td.Normal(m, v)\n",
    "        \n",
    "\n",
    "# dimensionality of model \n",
    "Dx = 2\n",
    "Dz = 2\n",
    "Dh = 100\n",
    "\n",
    "# label ratio, only nu\n",
    "nu = 1\n",
    "\n",
    "emb = EBM(Dx, Dz, Dh)   \n",
    "\n",
    "recog   = Posterior(Dx, Dh, Dz)\n",
    "        \n",
    "# noise distribution\n",
    "p_noise = td.Normal(torch.zeros(Dx), torch.ones(Dx)*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VNCE_loss(xs):\n",
    "    \n",
    "    '''\n",
    "    Variational NCE objective\n",
    "    '''\n",
    "    m = 5\n",
    "    xs = torch.as_tensor(xs, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    qs_x = recog(xs)\n",
    "    zs_x = qs_x.rsample([m])\n",
    "    E_x = emb(xs, zs_x)\n",
    "    ys = p_noise.sample([xs.shape[0]])\n",
    "    \n",
    "    qs_y = recog(ys)\n",
    "    zs_y = qs_y.rsample([m])\n",
    "    E_y = emb(ys, zs_y)\n",
    "    # Equation 13, \n",
    "    # http://proceedings.mlr.press/v89/rhodes19a/rhodes19a.pdf\n",
    "    sig = lambda x: torch.log(1. + x )\n",
    "    bound = - sig(nu * (p_noise.log_prob(xs).sum(-1) + qs_x.log_prob(zs_x).sum(-1) - E_x ).exp()).mean(0) -\\\n",
    "             nu * sig( 1. / nu  / p_noise.log_prob(ys).sum(-1).exp() * (E_y - qs_y.log_prob(zs_y).sum(-1)).exp().mean(0) )\n",
    "    \n",
    "    return -bound.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(emb.parameters(), lr=1e-4)\n",
    "opt_recog = torch.optim.Adam(recog.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(30000):\n",
    "    \n",
    "    loss = VNCE_loss(x)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    opt_recog.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt_recog.step()\n",
    "    \n",
    "    losses += loss.item(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a grid for numerical normalisation\n",
    "from itertools import product\n",
    "ngrid = 50\n",
    "grid = torch.linspace(-10,10,ngrid)\n",
    "xz_eval = torch.tensor(list(product(*[grid]*4)))\n",
    "x_eval = xz_eval[:,:2]\n",
    "z_eval = xz_eval[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true log density\n",
    "E_true = p.logpdf_multiple(torch.tensor(list(product(*[grid]*2))))\n",
    "E_true -= E_true.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EBM log density\n",
    "E_eval = emb(x_eval, z_eval).reshape(ngrid,ngrid,ngrid,ngrid).exp().detach()\n",
    "E_eval /= E_eval.sum()\n",
    "E_eval = E_eval.sum(-1).sum(-1)\n",
    "E_eval.log_()\n",
    "E_eval -= E_eval.max()\n",
    "# E_eval = E_eval.sum(-1).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(E):\n",
    "    if isinstance(E, np.ndarray):\n",
    "        E = np.exp(E)\n",
    "    else:\n",
    "        E = E.exp()\n",
    "    E /= E.sum()\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2,figsize=(6,6), sharex=True, sharey=True)\n",
    "\n",
    "\n",
    "ax = axes[0,0]\n",
    "ax.pcolor(grid, grid,E_true.reshape(ngrid,ngrid), shading='auto', vmin=-10, vmax=0)\n",
    "ax.scatter(x[:,1], x[:,0], c=\"r\", s=1, alpha=0.05)\n",
    "\n",
    "\n",
    "ax = axes[1,0]\n",
    "ax.pcolor(grid, grid,normalise(E_true).reshape(ngrid,ngrid), shading='auto')\n",
    "\n",
    "ax = axes[0,1]\n",
    "ax.pcolor(grid, grid,E_eval,shading='auto', vmin=-10, vmax=0, )\n",
    "ax.scatter(x[:,1], x[:,0], c=\"r\", s=1, alpha=0.05)\n",
    "\n",
    "ax = axes[1,1]\n",
    "ax.pcolor(grid, grid,normalise(E_eval),shading='auto' )\n",
    "ax.scatter(x[:,1], x[:,0], c=\"r\", s=1, alpha=0.0)\n",
    "\n",
    "\n",
    "\n",
    "axes[0,0].set_ylabel(\"logp\")\n",
    "axes[1,0].set_ylabel(\"p\")\n",
    "\n",
    "axes[0,0].set_title(\"data\")\n",
    "axes[0,1].set_title(\"VNCE\")\n",
    "\n",
    "axes[0,0].set_xlim(-10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (score_EM)",
   "language": "python",
   "name": "score_em"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
